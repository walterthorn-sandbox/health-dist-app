/**
 * Voice WebSocket Server
 *
 * Handles Twilio Media Streams and connects to OpenAI Realtime API
 * for conversational food permit application collection.
 *
 * Run with: tsx server/voice-server.ts
 */

import Fastify from "fastify";
import FastifyWebSocket from "@fastify/websocket";
import FastifyFormBody from "@fastify/formbody";
import Ably from "ably";
import { config } from "dotenv";
import * as path from "path";
import OpenAI from "openai";
import WebSocket from "ws";
import { ConversationSession } from "./braintrust-logger";

// Load environment variables
config({ path: path.join(__dirname, "..", ".env.local") });

const PORT = process.env.VOICE_SERVER_PORT || 5050;
const NEXTJS_PORT = process.env.PORT || 3000;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const ABLY_API_KEY = process.env.ABLY_API_KEY;

if (!OPENAI_API_KEY) {
  console.error("‚ùå OPENAI_API_KEY environment variable is required");
  process.exit(1);
}

if (!ABLY_API_KEY) {
  console.error("‚ùå ABLY_API_KEY environment variable is required");
  process.exit(1);
}

// Initialize OpenAI client
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

// Initialize Ably client
const ablyClient = new Ably.Realtime(ABLY_API_KEY);

// Create Fastify server
const fastify = Fastify({
  logger: true,
});

// Register plugins
fastify.register(FastifyWebSocket);
fastify.register(FastifyFormBody);

// In-memory session storage (map sessionId -> session data)
const sessions = new Map<string, {
  sessionId: string;
  channelName: string;
  formData: Record<string, unknown>;
}>();

/**
 * WebSocket endpoint for Twilio Media Streams
 * URL: ws://localhost:5050/media-stream
 */
fastify.register(async (fastify) => {
  fastify.get("/media-stream", { websocket: true }, async (twilioWs, req) => {
    console.log(`üìû New WebSocket connection from Twilio`);

    let sessionId = "unknown";
    let sessionData: any = null;
    let ablyChannel: any = null;
    let openaiWs: WebSocket | null = null;
    let streamSid: string | null = null;
    let braintrustSession: ConversationSession | null = null;

    // Listen for messages from Twilio
    twilioWs.on("message", async (data: Buffer) => {
      try {
        const message = JSON.parse(data.toString());

        // Handle start event - this is when we set up the OpenAI connection
        if (message.event === "start") {
          streamSid = message.start.streamSid;
          sessionId = message.start.customParameters?.sessionId || "unknown";

          console.log(`üîë Session ID: ${sessionId}`);
          console.log(`üì° Stream SID: ${streamSid}`);

          // Set up session data
          sessionData = sessions.get(sessionId);
          if (!sessionData) {
            sessionData = {
              sessionId,
              channelName: `session:${sessionId}`,
              formData: {},
            };
            sessions.set(sessionId, sessionData);
          }

          // Create Ably channel for this session
          ablyChannel = ablyClient.channels.get(sessionData.channelName);
          console.log(`üì° Ably channel name: ${sessionData.channelName}`);

          // Initialize Braintrust session tracking
          braintrustSession = new ConversationSession(sessionId);

          console.log(`ü§ñ Creating OpenAI Realtime session for ${sessionId}...`);

          // Create OpenAI Realtime session
          const session = await openai.beta.realtime.sessions.create({
            model: "gpt-4o-realtime-preview-2024-12-17",
            voice: "alloy",
            instructions: `You are a helpful assistant for the Riverside County Health District.
Your job is to collect information for a food establishment permit application through a conversational voice call.

You should greet the caller and ask for the following information in a natural, friendly way:
1. Establishment name
2. Street address
3. Establishment phone number
4. Establishment email
5. Owner/operator name
6. Owner phone number
7. Owner email
8. Type of establishment (restaurant, food truck, bakery, etc.)
9. Planned opening date

IMPORTANT INSTRUCTIONS:
- Ask ONE question at a time
- After the user answers, IMMEDIATELY acknowledge their answer and ask the next question - do NOT wait for them to prompt you
- When you collect a piece of information, call the updateField function AND then continue the conversation by asking the next question
- Keep the conversation flowing naturally - don't leave long pauses
- After collecting all information, call the submitApplication function to complete the process`,
            input_audio_format: "g711_ulaw",
            output_audio_format: "g711_ulaw",
            turn_detection: {
              type: "server_vad",
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 500,
            },
            tools: [
              {
                type: "function",
                name: "updateField",
                description: "Update a field in the food permit application form. This will broadcast the update to the user's mobile device in real-time.",
                parameters: {
                  type: "object",
                  properties: {
                    field: {
                      type: "string",
                      enum: [
                        "establishmentName",
                        "streetAddress",
                        "establishmentPhone",
                        "establishmentEmail",
                        "ownerName",
                        "ownerPhone",
                        "ownerEmail",
                        "establishmentType",
                        "plannedOpeningDate",
                      ],
                    },
                    value: {
                      type: "string",
                    },
                  },
                  required: ["field", "value"],
                },
              },
              {
                type: "function",
                name: "submitApplication",
                description: "Submit the completed food permit application",
                parameters: {
                  type: "object",
                  properties: {
                    trackingId: {
                      type: "string",
                    },
                  },
                  required: ["trackingId"],
                },
              },
            ],
          });

          console.log(`‚úÖ OpenAI session created`);

          // Connect to OpenAI Realtime WebSocket
          const url = `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17`;
          openaiWs = new WebSocket(url, {
            headers: {
              "Authorization": `Bearer ${OPENAI_API_KEY}`,
              "OpenAI-Beta": "realtime=v1",
            },
          });

          // Handle OpenAI WebSocket connection
          openaiWs.on("open", () => {
            console.log(`üîå Connected to OpenAI Realtime API`);

            // Send session update to configure the session
            openaiWs!.send(JSON.stringify({
              type: "session.update",
              session: {
                turn_detection: {
                  type: "server_vad",
                  threshold: 0.5,
                  prefix_padding_ms: 300,
                  silence_duration_ms: 1000,  // Wait 1000ms of silence before assuming user is done
                },
                input_audio_format: "g711_ulaw",
                output_audio_format: "g711_ulaw",
                input_audio_transcription: {
                  model: "whisper-1",  // Enable transcription of user speech
                },
                voice: "alloy",
                instructions: session.instructions,
                tools: session.tools,
                tool_choice: "auto",
                temperature: 0.8,
                modalities: ["text", "audio"],
              },
            }));

            console.log(`üìù Session configured`);

            // Send an initial message to trigger the agent to greet the caller
            setTimeout(() => {
              console.log(`üëã Triggering initial greeting`);
              openaiWs!.send(JSON.stringify({
                type: "response.create",
                response: {
                  modalities: ["text", "audio"],
                  instructions: "Greet the caller and start the conversation by asking for the establishment name.",
                },
              }));
            }, 500);  // Small delay to ensure session.update is processed first
          });

          // Handle messages from OpenAI
          openaiWs.on("message", async (data: Buffer) => {
            try {
              const message = JSON.parse(data.toString());

              // Log important events for debugging and Braintrust
              if (message.type === "response.done" || message.type === "response.created" ||
                  message.type === "input_audio_buffer.speech_started" || message.type === "input_audio_buffer.speech_stopped") {
                console.log(`üì° OpenAI event: ${message.type}`);
              }

              // Track conversation turns in Braintrust with transcripts
              if (message.type === "conversation.item.input_audio_transcription.completed") {
                // User speech transcript
                const transcript = message.transcript;
                braintrustSession?.logUserTurn(transcript);
                console.log(`üìù User transcript: ${transcript}`);
              }

              // Track agent text from audio transcript
              if (message.type === "response.audio_transcript.done") {
                const transcript = message.transcript;
                braintrustSession?.logAgentTurn(transcript);
                console.log(`üìù Agent transcript: ${transcript}`);
              }

              if (message.type === "response.done") {
                // Try to extract text from the response
                const response = message.response;
                let agentText = "";

                // Check all output items for text or audio transcripts
                if (response?.output) {
                  for (const item of response.output) {
                    if (item.content) {
                      for (const content of item.content) {
                        if (content.type === "text" && content.text) {
                          agentText += content.text + " ";
                        } else if (content.type === "audio" && content.transcript) {
                          agentText += content.transcript + " ";
                        }
                      }
                    }
                  }
                }

                if (agentText.trim()) {
                  console.log(`üìù Agent text from response.done: ${agentText.trim()}`);
                }
              }

              // Forward audio from OpenAI to Twilio
              if (message.type === "response.audio.delta" && message.delta) {
                const audioPayload = {
                  event: "media",
                  streamSid: streamSid,
                  media: {
                    payload: message.delta,
                  },
                };
                twilioWs.send(JSON.stringify(audioPayload));
              }

              // When speech is detected as stopped, the server VAD will automatically trigger a response
              // No manual intervention needed due to turn_detection configuration

              // Handle function calls
              if (message.type === "response.function_call_arguments.done") {
                const functionName = message.name;
                const args = JSON.parse(message.arguments);

                console.log(`üîß Function call: ${functionName}`, args);

                if (functionName === "updateField") {
                  // Update session data
                  if (sessionData) {
                    sessionData.formData[args.field] = args.value;
                  }

                  // Broadcast update via Ably
                  try {
                    console.log(`üì° Publishing to channel: ${sessionData.channelName}, event: field-update`);
                    await ablyChannel?.publish("field-update", {
                      field: args.field,
                      value: args.value,
                      timestamp: Date.now(),
                    });
                    console.log(`üì§ Broadcast field update: ${args.field} = ${args.value}`);

                    // Track in Braintrust
                    braintrustSession?.logFunctionCall("updateField", args, { success: true });
                  } catch (error) {
                    console.error(`‚ùå Failed to broadcast update:`, error);
                    braintrustSession?.logFunctionCall("updateField", args, { success: false, error: String(error) });
                  }

                  // Send function call result back to OpenAI
                  openaiWs!.send(JSON.stringify({
                    type: "conversation.item.create",
                    item: {
                      type: "function_call_output",
                      call_id: message.call_id,
                      output: JSON.stringify({ success: true, field: args.field, value: args.value }),
                    },
                  }));

                  // Trigger a new response to continue the conversation
                  openaiWs!.send(JSON.stringify({
                    type: "response.create",
                  }));
                  console.log(`üîÑ Triggered new response after updateField`);
                } else if (functionName === "submitApplication") {
                  // Broadcast completion via Ably
                  try {
                    await ablyChannel?.publish("session-complete", {
                      trackingId: args.trackingId,
                      timestamp: Date.now(),
                    });
                    console.log(`‚úÖ Application submitted: ${args.trackingId}`);

                    // Mark session as completed in Braintrust
                    braintrustSession?.markCompleted();
                    braintrustSession?.logFunctionCall("submitApplication", args, { success: true });
                  } catch (error) {
                    console.error(`‚ùå Failed to broadcast completion:`, error);
                    braintrustSession?.logFunctionCall("submitApplication", args, { success: false, error: String(error) });
                  }

                  // Send function call result back to OpenAI
                  openaiWs!.send(JSON.stringify({
                    type: "conversation.item.create",
                    item: {
                      type: "function_call_output",
                      call_id: message.call_id,
                      output: JSON.stringify({ success: true, trackingId: args.trackingId }),
                    },
                  }));

                  // Clean up session
                  sessions.delete(sessionId);
                }
              }

              // Log other important events
              if (message.type === "error") {
                console.error(`‚ùå OpenAI error:`, message);
              }
            } catch (error) {
              console.error(`Error processing OpenAI message:`, error);
            }
          });

          openaiWs.on("error", (error) => {
            console.error(`‚ùå OpenAI WebSocket error:`, error);
          });

          openaiWs.on("close", () => {
            console.log(`üì¥ OpenAI WebSocket closed`);
          });
        }

        // Handle media event - forward audio to OpenAI
        if (message.event === "media" && openaiWs && openaiWs.readyState === WebSocket.OPEN) {
          const audioPayload = {
            type: "input_audio_buffer.append",
            audio: message.media.payload,
          };
          openaiWs.send(JSON.stringify(audioPayload));
        }

        // Handle stop event
        if (message.event === "stop") {
          console.log(`‚èπÔ∏è  Stream stopped`);
          if (openaiWs) {
            openaiWs.close();
          }
        }
      } catch (error) {
        // Not a JSON message or error parsing - ignore
      }
    });

    // Handle Twilio WebSocket close
    twilioWs.on("close", async () => {
      console.log(`üì¥ Twilio WebSocket closed - Session: ${sessionId}`);
      if (openaiWs) {
        openaiWs.close();
      }
      sessions.delete(sessionId);

      // Flush Braintrust session data
      if (braintrustSession) {
        await braintrustSession.flush();
      }
    });

    twilioWs.on("error", (error) => {
      console.error(`‚ùå Twilio WebSocket error:`, error);
    });
  });
});

// Health check endpoint
fastify.get("/health", async () => {
  return { status: "ok", sessions: sessions.size };
});

// Proxy all other requests to Next.js
fastify.all("/*", async (request, reply) => {
  try {
    const url = `http://localhost:${NEXTJS_PORT}${request.url}`;

    // Prepare request body based on content type
    let body: string | undefined;
    if (request.method !== "GET" && request.method !== "HEAD" && request.body) {
      const contentType = request.headers["content-type"];
      if (contentType?.includes("application/x-www-form-urlencoded")) {
        // Convert parsed form body back to URL-encoded string
        const params = new URLSearchParams(request.body as Record<string, string>);
        body = params.toString();
      } else if (contentType?.includes("application/json")) {
        body = JSON.stringify(request.body);
      } else {
        // For other content types, try to stringify
        body = typeof request.body === "string" ? request.body : JSON.stringify(request.body);
      }
    }

    const response = await fetch(url, {
      method: request.method,
      headers: request.headers as HeadersInit,
      body,
    });

    const responseContentType = response.headers.get("content-type");
    const responseBody = responseContentType?.includes("application/json")
      ? await response.json()
      : await response.text();

    // Forward all response headers
    const responseHeaders = Object.fromEntries(response.headers.entries());

    reply
      .code(response.status)
      .headers(responseHeaders)
      .send(responseBody);
  } catch (error) {
    console.error("Proxy error:", error);
    reply.code(502).send({ error: "Bad Gateway - Next.js server not available" });
  }
});

// Start server
const start = async () => {
  try {
    await fastify.listen({ port: Number(PORT), host: "0.0.0.0" });
    console.log(`
üéôÔ∏è  Voice WebSocket Server is running!
üìû WebSocket endpoint: ws://localhost:${PORT}/media-stream
üè• Health check: http://localhost:${PORT}/health
üîÑ Proxying HTTP requests to Next.js on port ${NEXTJS_PORT}

Configure Twilio Media Streams to point to:
wss://your-ngrok-url.ngrok-free.app/media-stream

Use ngrok for local development:
ngrok http ${PORT}

IMPORTANT: Point ngrok to port ${PORT} (this server), not ${NEXTJS_PORT}
This server will proxy all web traffic to Next.js while handling WebSocket connections.
    `);
  } catch (err) {
    fastify.log.error(err);
    process.exit(1);
  }
};

start();
